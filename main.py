import os
import json
import gspread
import re
from playwright.sync_api import sync_playwright

def run_total_ranking():
    print("ğŸš€ [í†µí•© ë­í‚¹ ì‹œìŠ¤í…œ] ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")
    
    # 1. êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ë° ì´ˆê¸°í™”
    try:
        creds = json.loads(os.environ['GOOGLE_CREDENTIALS'])
        gc = gspread.service_account_from_dict(creds)
        # âš ï¸ ë³¸ì¸ì˜ ì‹œíŠ¸ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”
        sh = gc.open_by_key("1c2ax0-3t70NxvxL-cXeOCz9NYnSC9OhrzC0IOWSe5Lc").sheet1
        
        # í—¤ë” ì‘ì„± ë° ê¸°ì¡´ ë‚´ìš© ì´ˆê¸°í™”
        header = [["ìˆœìœ„", "í”Œë«í¼", "íƒ€ì´í‹€", "ì‘ê°€", "ì¥ë¥´", "ì¡°íšŒìˆ˜", "ì¸ë„¤ì¼", "ìˆ˜ì§‘ì¼"]]
        sh.clear()
        sh.update('A1', header)
        print("âœ… ì‹œíŠ¸ ì´ˆê¸°í™” ë° í—¤ë” ì‘ì„± ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ì‹œíŠ¸ ì—°ê²° ì‹¤íŒ¨: {e}")
        return

    with sync_playwright() as p:
        # ğŸ’¡ ë„¤ì´ë²„ ì°¨ë‹¨ì´ ì˜ì‹¬ë˜ë©´ headless=Falseë¡œ ë³€ê²½í•˜ì„¸ìš”.
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
        )

        # --- [STEP 1] ì¹´ì¹´ì˜¤í˜ì´ì§€ ìˆ˜ì§‘ ---
        print("\n--- [1/2] ì¹´ì¹´ì˜¤í˜ì´ì§€ ìˆ˜ì§‘ ë° ê¸°ë¡ ì‹œì‘ ---")
        k_page = context.new_page()
        try:
            k_page.goto("https://page.kakao.com/menu/10011/screen/94", wait_until="networkidle")
            k_page.wait_for_timeout(3000)
            links = k_page.eval_on_selector_all('a[href*="/content/"]', 'elements => elements.map(e => e.href)')
            unique_links = list(dict.fromkeys(links))[:20]

            for i, link in enumerate(unique_links):
                try:
                    dp = context.new_page()
                    dp.goto(link, wait_until="networkidle")
                    title = dp.locator('meta[property="og:title"]').get_attribute("content")
                    thumb = dp.locator('meta[property="og:image"]').get_attribute("content")
                    author = dp.locator('span.text-el-70.opacity-70').first.inner_text().strip()
                    
                    # ì¡°íšŒìˆ˜ ì¶”ì¶œ
                    body = dp.evaluate("() => document.body.innerText")
                    views = re.search(r'(\d+\.?\d*[ë§Œ|ì–µ])', body).group(1) if re.search(r'(\d+\.?\d*[ë§Œ|ì–µ])', body) else "-"
                    
                    # ì‹œíŠ¸ì— ì¦‰ì‹œ ê¸°ë¡
                    sh.append_row([f"{i+1}ìœ„", "ì¹´ì¹´ì˜¤í˜ì´ì§€", title, author, "ì¥ë¥´", views, thumb, "2026-02-25"])
                    print(f"   âœ… ì¹´ì¹´ì˜¤ {i+1}ìœ„ ê¸°ë¡ ì™„ë£Œ: {title}")
                    dp.close()
                except: continue
        except Exception as e:
            print(f"âŒ ì¹´ì¹´ì˜¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        k_page.close()

        # --- [STEP 2] ë„¤ì´ë²„ ì‹œë¦¬ì¦ˆ ìˆ˜ì§‘ ---
        print("\n--- [2/2] ë„¤ì´ë²„ ì‹œë¦¬ì¦ˆ ìˆ˜ì§‘ ë° ê¸°ë¡ ì‹œì‘ ---")
        n_page = context.new_page()
        try:
            n_page.goto("https://series.naver.com/novel/top100List.series", wait_until="networkidle")
            n_page.wait_for_timeout(5000)
            
            # ğŸ” ë´‡ ì°¨ë‹¨ ì—¬ë¶€ ì²´í¬
            page_content = n_page.content()
            bot_keywords = ["ì„œë¹„ìŠ¤ ì´ìš©ì´ ì œí•œë˜ì—ˆìŠµë‹ˆë‹¤", "ë¹„ì •ìƒì ì¸ ì ‘ê·¼", "Captcha", "ë¡œë´‡ì´ ì•„ë‹™ë‹ˆë‹¤"]
            
            if any(kw in page_content for kw in bot_keywords):
                print("ğŸš¨ [ê²½ê³ ] ë„¤ì´ë²„ê°€ ë´‡ìœ¼ë¡œ ê°ì§€í•˜ì—¬ ì°¨ë‹¨ì„ ê±¸ì—ˆìŠµë‹ˆë‹¤!")
                sh.append_row(["-", "ë„¤ì´ë²„ ì‹œë¦¬ì¦ˆ", "ë´‡ ì°¨ë‹¨ë¨", "ì—ëŸ¬", "-", "-", "-", "2026-02-25"])
            else:
                items = n_page.locator('ul.lst_list > li, .lst_list_wrap li').all()
                print(f"   ğŸ” ë„¤ì´ë²„ ë°œê²¬ëœ í•­ëª©: {len(items)}ê°œ")

                if len(items) == 0:
                    print("âš ï¸ ë°œê²¬ëœ í•­ëª©ì´ 0ê°œì…ë‹ˆë‹¤. (ì°¨ë‹¨ì€ ì•„ë‹ˆë‚˜ êµ¬ì¡° í™•ì¸ í•„ìš”)")
                
                for i, item in enumerate(items[:20]):
                    try:
                        # ì£¼ì‹  HTML ê¸°ë°˜ ì„ íƒì
                        title_el = item.locator('h3 a').first
                        title = title_el.inner_text().strip()
                        href = title_el.get_attribute('href')
                        author = item.locator('span.author').first.inner_text().strip()
                        thumb = item.locator('img').first.get_attribute('src')

                        # ìƒì„¸í˜ì´ì§€ ì¡°íšŒìˆ˜
                        detail_url = f"https://series.naver.com{href}"
                        dp = context.new_page()
                        dp.goto(detail_url, wait_until="domcontentloaded")
                        dp_text = dp.evaluate("() => document.body.innerText")
                        views = re.search(r'(\d+\.?\d*[ë§Œ|ì–µ])', dp_text).group(1) if re.search(r'(\d+\.?\d*[ë§Œ|ì–µ])', dp_text) else "-"
                        
                        # ì‹œíŠ¸ì— ì¦‰ì‹œ ê¸°ë¡
                        sh.append_row([f"{i+1}ìœ„", "ë„¤ì´ë²„ ì‹œë¦¬ì¦ˆ", title, author, "ì¥ë¥´", views, thumb, "2026-02-25"])
                        print(f"   âœ… ë„¤ì´ë²„ {i+1}ìœ„ ê¸°ë¡ ì™„ë£Œ: {title} ({views})")
                        dp.close()
                    except: continue
        except Exception as e:
            print(f"âŒ ë„¤ì´ë²„ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        n_page.close()

        browser.close()
        print("\nğŸŠ ëª¨ë“  ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    run_total_ranking()
