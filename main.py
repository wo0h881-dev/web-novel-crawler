import os
import json
import gspread
import re
from playwright.sync_api import sync_playwright

def run_total_ranking():
    print("ğŸš€ [ì§„ë‹¨ ì‹œì‘] í”„ë¡œì„¸ìŠ¤ë¥¼ ê°€ë™í•©ë‹ˆë‹¤...")
    
    # 1. ì‹œíŠ¸ ì ‘ì† ë‹¨ê³„ ì§„ë‹¨
    try:
        creds_raw = os.environ.get('GOOGLE_CREDENTIALS')
        if not creds_raw:
            print("âŒ ì—ëŸ¬: GOOGLE_CREDENTIALS í™˜ê²½ ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        creds = json.loads(creds_raw)
        gc = gspread.service_account_from_dict(creds)
        # âš ï¸ ë³¸ì¸ì˜ ì‹œíŠ¸ IDê°€ ë§ëŠ”ì§€ ë‹¤ì‹œ í™•ì¸!
        sh = gc.open_by_key("1c2ax0-3t70NxvxL-cXeOCz9NYnSC9OhrzC0IOWSe5Lc").sheet1
        print("âœ… êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ì„±ê³µ")
    except Exception as e:
        print(f"âŒ ì‹œíŠ¸ ì ‘ì† ë‹¨ê³„ ì—ëŸ¬: {e}")
        return

    # ì´ˆê¸°í™”
    header = [["ìˆœìœ„", "í”Œë«í¼", "íƒ€ì´í‹€", "ì‘ê°€", "ì¥ë¥´", "ì¡°íšŒìˆ˜", "ì¸ë„¤ì¼", "ìˆ˜ì§‘ì¼"]]
    sh.clear()
    sh.update('A1', header)

    with sync_playwright() as p:
        print("ğŸŒ ë¸Œë¼ìš°ì € ì‹¤í–‰ ì¤‘...")
        # ğŸ’¡ ì•„ë¬´ ë°˜ì‘ì´ ì—†ì„ ë• headless=Falseë¡œ ë°”ê¿”ì„œ ì°½ì´ ëœ¨ëŠ”ì§€ ë´ì•¼ í•©ë‹ˆë‹¤.
        browser = p.chromium.launch(headless=False) 
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
        )

        # --- [ì¹´ì¹´ì˜¤] ---
        print("--- [ì¹´ì¹´ì˜¤] ìˆ˜ì§‘ ì‹œì‘ ---")
        k_page = context.new_page()
        try:
            k_page.goto("https://page.kakao.com/menu/10011/screen/94", wait_until="load", timeout=30000)
            k_page.wait_for_timeout(3000)
            
            # ì¹´ì¹´ì˜¤ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
            links = k_page.eval_on_selector_all('a[href*="/content/"]', 'elements => elements.map(e => e.href)')
            unique_links = list(dict.fromkeys(links))[:20]
            print(f"   ì¹´ì¹´ì˜¤ ì‘í’ˆ {len(unique_links)}ê°œ ë°œê²¬")

            for i, link in enumerate(unique_links):
                dp = context.new_page()
                dp.goto(link, wait_until="load")
                title = dp.locator('meta[property="og:title"]').get_attribute("content")
                author = dp.locator('span.text-el-70.opacity-70').first.inner_text().strip()
                thumb = dp.locator('meta[property="og:image"]').get_attribute("content")
                
                # ì¥ë¥´ ë³µêµ¬ ë¡œì§
                genre = "-"
                g_elements = dp.locator('span.break-all.align-middle').all_inner_texts()
                genre = [g for g in g_elements if g != "ì›¹ì†Œì„¤"][0] if len(g_elements) > 1 else "-"

                body = dp.evaluate("() => document.body.innerText")
                views = re.search(r'(\d+\.?\d*[ë§Œ|ì–µ])', body).group(1) if re.search(r'(\d+\.?\d*[ë§Œ|ì–µ])', body) else "-"
                
                sh.append_row([f"{i+1}ìœ„", "ì¹´ì¹´ì˜¤í˜ì´ì§€", title, author, genre, views, thumb, "2026-02-25"])
                print(f"   âœ… ì¹´ì¹´ì˜¤ {i+1}ìœ„ ê¸°ë¡: {title}")
                dp.close()
        except Exception as e:
            print(f"âŒ ì¹´ì¹´ì˜¤ ê³¼ì • ì¤‘ ìƒì„¸ ì—ëŸ¬: {e}")

        # --- [ë„¤ì´ë²„] ---
        print("\n--- [ë„¤ì´ë²„] ìˆ˜ì§‘ ì‹œì‘ ---")
        n_page = context.new_page()
        try:
            # ì£¼ì†Œ ë’¤ì— íŒŒë¼ë¯¸í„°ë¥¼ ë¶™ì—¬ ì‹¤ì‹œê°„ ë­í‚¹ì„ ê°•ì œë¡œ í˜¸ì¶œ
            n_url = "https://series.naver.com/novel/top100List.series?rankingTypeCode=REALTIME&categoryCode=ALL"
            n_page.goto(n_url, wait_until="load", timeout=30000)
            n_page.wait_for_timeout(5000)

            # ë„¤ì´ë²„ ì°¨ë‹¨ í™•ì¸ìš© ìŠ¤í¬ë¦°ìƒ· (ì„ íƒ ì‚¬í•­)
            # n_page.screenshot(path="naver_check.png") 

            # ì„ íƒì ëŒ€í­ ë³´ê°•: ì£¼ì‹  HTMLì˜ 'comic_cont' í´ë˜ìŠ¤ë¥¼ ì§ì ‘ íƒ€ê²ŸíŒ…
            items = n_page.locator('.lst_list_wrap li, .lst_list li').all()
            print(f"   ğŸ” ë„¤ì´ë²„ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ê²°ê³¼: {len(items)}ê°œ ë°œê²¬")

            for i, item in enumerate(items[:20]):
                try:
                    # h3 ë‚´ë¶€ì˜ a íƒœê·¸ ì°¾ê¸°
                    target_a = item.locator('h3 a').first
                    title = target_a.inner_text().strip()
                    href = target_a.get_attribute('href')
                    author = item.locator('.author').first.inner_text().strip()
                    thumb = item.locator('img').first.get_attribute('src')
                    genre = item.locator('.genre').first.inner_text().strip() if item.locator('.genre').count() > 0 else "-"

                    # ìƒì„¸í˜ì´ì§€ ì´ë™
                    dp = context.new_page()
                    dp.goto(f"https://series.naver.com{href}", wait_until="load")
                    dp_text = dp.evaluate("() => document.body.innerText")
                    views = re.search(r'(\d+\.?\d*[ë§Œ|ì–µ])', dp_text).group(1) if re.search(r'(\d+\.?\d*[ë§Œ|ì–µ])', dp_text) else "-"
                    
                    sh.append_row([f"{i+1}ìœ„", "ë„¤ì´ë²„ ì‹œë¦¬ì¦ˆ", title, author, genre, views, thumb, "2026-02-25"])
                    print(f"   âœ… ë„¤ì´ë²„ {i+1}ìœ„ ê¸°ë¡: {title}")
                    dp.close()
                except Exception as e:
                    print(f"   âš ï¸ ë„¤ì´ë²„ {i+1}ìœ„ ê°œë³„ ìˆ˜ì§‘ ì‹¤íŒ¨")
                    continue
        except Exception as e:
            print(f"âŒ ë„¤ì´ë²„ ê³¼ì • ì¤‘ ìƒì„¸ ì—ëŸ¬: {e}")

        browser.close()
        print("\nğŸŠ ëª¨ë“  í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ")

if __name__ == "__main__":
    run_total_ranking()
