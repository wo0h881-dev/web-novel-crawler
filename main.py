import os
import json
import gspread
import re
import time
from playwright.sync_api import sync_playwright

def run_total_ranking():
    print("ğŸš€ [í†µí•© ë­í‚¹ ì‹œìŠ¤í…œ] ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")
    
    # 1. êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ë° ì´ˆê¸°í™”
    try:
        creds_raw = os.environ.get('GOOGLE_CREDENTIALS')
        if not creds_raw:
            print("âŒ ì—ëŸ¬: GOOGLE_CREDENTIALS í™˜ê²½ ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        creds = json.loads(creds_raw)
        gc = gspread.service_account_from_dict(creds)
        # âš ï¸ ë³¸ì¸ì˜ ì‹œíŠ¸ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”
        sh = gc.open_by_key("1c2ax0-3t70NxvxL-cXeOCz9NYnSC9OhrzC0IOWSe5Lc").sheet1
        
        # í—¤ë” ì‘ì„± ë° ì‹œíŠ¸ ë¹„ìš°ê¸°
        header = [["ìˆœìœ„", "í”Œë«í¼", "íƒ€ì´í‹€", "ì‘ê°€", "ì¥ë¥´", "ì¡°íšŒìˆ˜", "ì¸ë„¤ì¼", "ìˆ˜ì§‘ì¼"]]
        sh.clear()
        sh.update('A1', header)
        print("âœ… ì‹œíŠ¸ ì´ˆê¸°í™” ë° ì—°ê²° ì„±ê³µ")
    except Exception as e:
        print(f"âŒ ì‹œì‘ ë‹¨ê³„ ì˜¤ë¥˜: {e}")
        return

    with sync_playwright() as p:
        # ê¹ƒí—ˆë¸Œ ì•¡ì…˜ í™˜ê²½ì„ ìœ„í•œ ë¸Œë¼ìš°ì € ì„¤ì •
        browser = p.chromium.launch(headless=True)
        # ë„¤ì´ë²„ ì°¨ë‹¨ì„ í”¼í•˜ê¸° ìœ„í•œ ëª¨ë°”ì¼ ìœ ì € ì—ì´ì „íŠ¸ ì„¤ì •
        context = browser.new_context(
            user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 14_7_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Mobile/15E148 Safari/604.1",
            viewport={'width': 375, 'height': 812}
        )

        # --- [STEP 1] ì¹´ì¹´ì˜¤í˜ì´ì§€ ìˆ˜ì§‘ ---
        print("\n--- [1/2] ì¹´ì¹´ì˜¤í˜ì´ì§€ ìˆ˜ì§‘ ì‹œì‘ ---")
        k_page = context.new_page()
        try:
            k_page.goto("https://page.kakao.com/menu/10011/screen/94", wait_until="load", timeout=60000)
            k_page.wait_for_timeout(3000) # ë¦¬ìŠ¤íŠ¸ ë¡œë”© ëŒ€ê¸°
            
            # ì‘í’ˆ ìƒì„¸ ë§í¬ ì¶”ì¶œ
            links = k_page.eval_on_selector_all('a[href*="/content/"]', 'elements => elements.map(e => e.href)')
            unique_links = list(dict.fromkeys(links))[:20]
            print(f"   ğŸ” ì¹´ì¹´ì˜¤ ì‘í’ˆ {len(unique_links)}ê°œ ë°œê²¬")

            for i, link in enumerate(unique_links):
                try:
                    dp = context.new_page()
                    dp.goto(link, wait_until="load")
                    title = dp.locator('meta[property="og:title"]').get_attribute("content")
                    thumb = dp.locator('meta[property="og:image"]').get_attribute("content")
                    author = dp.locator('span.text-el-70.opacity-70').first.inner_text().strip()
                    
                    # ì¥ë¥´ ì¶”ì¶œ ë¡œì§
                    genre = "-"
                    g_elements = dp.locator('span.break-all.align-middle').all_inner_texts()
                    if len(g_elements) > 1:
                        genre = [g for g in g_elements if g != "ì›¹ì†Œì„¤"][0]
                    elif len(g_elements) == 1:
                        genre = g_elements[0].replace("ì›¹ì†Œì„¤", "").strip()

                    # ì¡°íšŒìˆ˜ ì¶”ì¶œ (ì •ê·œì‹)
                    body = dp.evaluate("() => document.body.innerText")
                    views = re.search(r'(\d+\.?\d*[ë§Œ|ì–µ])', body).group(1) if re.search(r'(\d+\.?\d*[ë§Œ|ì–µ])', body) else "-"
                    
                    sh.append_row([f"{i+1}ìœ„", "ì¹´ì¹´ì˜¤í˜ì´ì§€", title, author, genre, views, thumb, "2026-02-25"])
                    print(f"   âœ… ì¹´ì¹´ì˜¤ {i+1}ìœ„ ì™„ë£Œ: {title}")
                    dp.close()
                except: continue
        except Exception as e:
            print(f"âŒ ì¹´ì¹´ì˜¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        k_page.close()

        # --- [STEP 2] ë„¤ì´ë²„ ì‹œë¦¬ì¦ˆ ìˆ˜ì§‘ (ëª¨ë°”ì¼ êµ¬ì¡° ìµœì í™”) ---
        print("\n--- [2/2] ë„¤ì´ë²„ ì‹œë¦¬ì¦ˆ ìˆ˜ì§‘ ì‹œì‘ ---")
        n_page = context.new_page()
        try:
            # ì‹¤ì‹œê°„ ë­í‚¹ ëª¨ë°”ì¼ ì£¼ì†Œ
            n_url = "https://m.series.naver.com/novel/top100List.series?rankingTypeCode=REALTIME&categoryCode=ALL"
            n_page.goto(n_url, wait_until="networkidle", timeout=60000)
            
            # ğŸ’¡ í•µì‹¬: 0ê°œê°€ ëœ¨ì§€ ì•Šë„ë¡ ë°ì´í„°ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ì¶©ë¶„íˆ ëŒ€ê¸°
            n_page.wait_for_selector("a.comic_top_ba, .lst_list > li", timeout=15000)
            n_page.evaluate("window.scrollTo(0, document.body.scrollHeight/2)")
            time.sleep(2)

            # ë³´ë‚´ì£¼ì‹  HTML êµ¬ì¡°ì— ê¸°ë°˜í•œ ì„ íƒì ì ìš©
            items = n_page.locator('a.comic_top_ba, ul.lst_list > li').all()
            print(f"   ğŸ” ë„¤ì´ë²„ ë°œê²¬ í•­ëª©: {len(items)}ê°œ")

            for i, item in enumerate(items[:20]):
                try:
                    # 1. ì œëª© (h5.tit ë˜ëŠ” strongì—ì„œ ì¶”ì¶œ)
                    if item.locator('h5.tit').count() > 0:
                        raw_title = item.locator('h5.tit').inner_text()
                        # "ìƒˆë¡œìš´ ì—í”¼ì†Œë“œ", "series edition" ë“± ë¶ˆí•„ìš”í•œ íƒœê·¸ í…ìŠ¤íŠ¸ ì œê±°
                        title = raw_title.replace("ìƒˆë¡œìš´ ì—í”¼ì†Œë“œ", "").replace("series edition", "").strip()
                    else:
                        title = item.locator('strong').first.inner_text().strip()

                    # 2. ì‘ê°€ (span.author)
                    author = item.locator('span.author').first.inner_text().strip()

                    # 3. ì¸ë„¤ì¼
                    thumb_el = item.locator('img').first
                    thumb = thumb_el.get_attribute('src') or thumb_el.get_attribute('data-src')

                    # 4. ì¡°íšŒìˆ˜ ìˆ˜ì§‘ì„ ìœ„í•´ ìƒì„¸ í˜ì´ì§€ ì´ë™
                    href = item.get_attribute('href')
                    if not href:
                        href = item.locator('a').first.get_attribute('href')
                    
                    dp = context.new_page()
                    # ìƒì„¸í˜ì´ì§€ëŠ” PC ë²„ì „ì´ í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ë” ê¹”ë”í•¨
                    dp.goto(f"https://series.naver.com{href}", wait_until="domcontentloaded")
                    dp_text = dp.evaluate("() => document.body.innerText")
                    views_match = re.search(r'(\d+\.?\d*[ë§Œ|ì–µ])', dp_text)
                    views = views_match.group(1) if views_match else "-"
                    
                    # ì¥ë¥´ (ë¦¬ìŠ¤íŠ¸ì— ìˆì„ ê²½ìš° ê°€ì ¸ì˜¤ê³  ì—†ìœ¼ë©´ ìƒì„¸í˜ì´ì§€ ë¶„ì„)
                    genre = item.locator('.genre').first.inner_text().strip() if item.locator('.genre').count() > 0 else "ì›¹ì†Œì„¤"

                    sh.append_row([f"{i+1}ìœ„", "ë„¤ì´ë²„ ì‹œë¦¬ì¦ˆ", title, author, genre, views, thumb, "2026-02-25"])
                    print(f"   âœ… ë„¤ì´ë²„ {i+1}ìœ„ ì™„ë£Œ: {title} ({views})")
                    dp.close()
                    
                    time.sleep(0.5) # ì„œë²„ ë¶€í•˜ ë°©ì§€
                except Exception as e:
                    print(f"   âš ï¸ ë„¤ì´ë²„ {i+1}ìœ„ ê°œë³„ ì˜¤ë¥˜: {e}")
                    continue
        except Exception as e:
            print(f"âŒ ë„¤ì´ë²„ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        browser.close()
        print("\nğŸŠ ëª¨ë“  ì‘ì—…ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    run_total_ranking()
