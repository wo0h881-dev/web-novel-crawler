import os
import json
import gspread
import re
from playwright.sync_api import sync_playwright

def get_kakao_data(context):
    print("      [1/2] ì¹´ì¹´ì˜¤í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...")
    data = []
    page = context.new_page()
    url = "https://page.kakao.com/menu/10011/screen/94"
    try:
        page.goto(url, wait_until="networkidle")
        page.wait_for_timeout(3000)
        links = page.eval_on_selector_all('a[href*="/content/"]', 'elements => elements.map(e => e.href)')
        unique_links = []
        for l in links:
            if l not in unique_links: unique_links.append(l)
        
        for i, link in enumerate(unique_links[:20]):
            try:
                d_page = context.new_page()
                d_page.goto(link, wait_until="networkidle")
                title = d_page.locator('meta[property="og:title"]').get_attribute("content")
                thumbnail = d_page.locator('meta[property="og:image"]').get_attribute("content")
                author = d_page.locator('span.text-el-70.opacity-70').first.inner_text().strip()
                
                genre = "-"
                genre_elements = d_page.locator('span.break-all.align-middle').all_inner_texts()
                if len(genre_elements) > 1:
                    genre = [g for g in genre_elements if g != "ì›¹ì†Œì„¤"][0]
                elif len(genre_elements) == 1:
                    genre = genre_elements[0].replace("ì›¹ì†Œì„¤", "").strip()

                view_match = re.search(r'(\d+\.?\d*[ë§Œ|ì–µ])', d_page.evaluate("() => document.body.innerText"))
                views = view_match.group(1) if view_match else "-"
                
                data.append([f"{i+1}ìœ„", "ì¹´ì¹´ì˜¤í˜ì´ì§€", title, author, genre, views, thumbnail, "2026-02-25"])
                d_page.close()
            except: continue
    except Exception as e: print(f"âŒ ì¹´ì¹´ì˜¤ ìˆ˜ì§‘ ì¤‘ë‹¨: {e}")
    page.close()
    return data

def get_naver_data(context):
    print("      [2/2] ë„¤ì´ë²„ ì‹œë¦¬ì¦ˆ ìˆ˜ì§‘ ì¤‘...")
    data = []
    page = context.new_page()
    # âš ï¸ ë„¤ì´ë²„ ì°¨ë‹¨ ìš°íšŒë¥¼ ìœ„í•´ ëª¨ë°”ì¼ ë²„ì „ ì£¼ì†Œë¥¼ ì‚¬ìš©í•´ë´…ë‹ˆë‹¤. (ë” ê°€ë²¼ì›€)
    url = "https://m.series.naver.com/novel/top100List.series?rankingTypeCode=REALTIME&categoryCode=ALL"
    
    try:
        page.goto(url, wait_until="load")
        page.wait_for_timeout(5000) # ë¡œë”© ëŒ€ê¸° ì‹œê°„ ëŒ€í­ ì¦ê°€

        # ì„ íƒìë¥¼ ë” ë„“ê²Œ ì¡ìŠµë‹ˆë‹¤. (ëª¨ë°”ì¼/PC ê³µìš© ëŒ€ì‘)
        items = page.locator('ul.lst_list > li, div.lst_list_wrap li').all()
        print(f"      ğŸ” ë°œê²¬ëœ ë„¤ì´ë²„ ì‘í’ˆ ìˆ˜: {len(items)}ê°œ")

        for i, item in enumerate(items[:20]):
            try:
                # ì œëª©/ì‘ê°€/ë§í¬ ì¶”ì¶œ
                title_link = item.locator('a').first
                title = title_link.inner_text().split('\n')[0].strip()
                href = title_link.get_attribute('href')
                
                author = item.locator('.author, .writer').first.inner_text().strip()
                thumbnail = item.locator('img').first.get_attribute("src")
                
                # ìƒì„¸í˜ì´ì§€ ì¡°íšŒìˆ˜
                detail_url = f"https://series.naver.com{href}" if href.startswith('/') else href
                d_page = context.new_page()
                d_page.goto(detail_url, wait_until="domcontentloaded")
                d_page.wait_for_timeout(2000)
                
                # <span>40.4ë§Œ</span> ì°¾ê¸°
                views = "-"
                all_text = d_page.evaluate("() => document.body.innerText")
                view_match = re.search(r'(\d+\.?\d*[ë§Œ|ì–µ])', all_text)
                if view_match: views = view_match.group(1)
                
                data.append([f"{i+1}ìœ„", "ë„¤ì´ë²„ ì‹œë¦¬ì¦ˆ", title, author, "ì¥ë¥´", views, thumbnail, "2026-02-25"])
                d_page.close()
            except: continue
    except Exception as e: print(f"âŒ ë„¤ì´ë²„ ìˆ˜ì§‘ ì¤‘ë‹¨: {e}")
    page.close()
    return data

def run_total_ranking():
    print("ğŸš€ í†µí•© ë­í‚¹ ìˆ˜ì§‘ ì‹œì‘ (ì¹´ì¹´ì˜¤ ìš°ì„  í™•ë³´)")
    try:
        creds = json.loads(os.environ['GOOGLE_CREDENTIALS'])
        gc = gspread.service_account_from_dict(creds)
        sh = gc.open_by_key("1c2ax0-3t70NxvxL-cXeOCz9NYnSC9OhrzC0IOWSe5Lc").sheet1
    except: return

    with sync_playwright() as p:
        # âš ï¸ headless=Trueì—¬ë„ ì‘ë™í•˜ê²Œë” ì„¤ì •ì„ ë” ì •êµí•˜ê²Œ í•¨
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 14_7_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Mobile/15E148 Safari/604.1",
            viewport={'width': 375, 'height': 812}
        )
        
        kakao_res = get_kakao_data(context)
        naver_res = get_naver_data(context)
        
        # ë°ì´í„° í•©ì¹˜ê¸°
        header = [["ìˆœìœ„", "í”Œë«í¼", "íƒ€ì´í‹€", "ì‘ê°€", "ì¥ë¥´", "ì¡°íšŒìˆ˜", "ì¸ë„¤ì¼", "ìˆ˜ì§‘ì¼"]]
        final_list = header + kakao_res + naver_res
        
        if len(final_list) > 1: # ë°ì´í„°ê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
            sh.clear()
            sh.update('A1', final_list)
            print(f"ğŸŠ ì™„ë£Œ! ì¹´ì¹´ì˜¤({len(kakao_res)}ê±´), ë„¤ì´ë²„({len(naver_res)}ê±´) ì €ì¥ë¨.")
        else:
            print("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ì–´ ì‹œíŠ¸ë¥¼ ì—…ë°ì´íŠ¸í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        browser.close()

if __name__ == "__main__":
    run_total_ranking()
