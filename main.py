import os
import json
import gspread
import re
from playwright.sync_api import sync_playwright

def get_kakao_data(context):
    print("      ì¹´ì¹´ì˜¤í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...")
    data = []
    page = context.new_page()
    url = "https://page.kakao.com/menu/10011/screen/94"
    page.goto(url, wait_until="networkidle")
    page.wait_for_timeout(3000)
    
    links = page.eval_on_selector_all('a[href*="/content/"]', 'elements => elements.map(e => e.href)')
    unique_links = []
    for link in links:
        if link not in unique_links: unique_links.append(link)
    
    for i, link in enumerate(unique_links[:20]):
        try:
            d_page = context.new_page()
            d_page.goto(link, wait_until="networkidle")
            d_page.wait_for_timeout(1500)
            title = d_page.locator('meta[property="og:title"]').get_attribute("content")
            thumbnail = d_page.locator('meta[property="og:image"]').get_attribute("content")
            author = d_page.locator('span.text-el-70.opacity-70').first.inner_text().strip()
            
            # ì¥ë¥´ ì •ì œ
            genre = "-"
            genre_elements = d_page.locator('span.break-all.align-middle').all_inner_texts()
            if len(genre_elements) > 1:
                genre = [g for g in genre_elements if g != "ì›¹ì†Œì„¤"][0]
            elif len(genre_elements) == 1:
                genre = genre_elements[0].replace("ì›¹ì†Œì„¤", "").strip()

            view_match = re.search(r'(\d+\.?\d*[ë§Œ|ì–µ])', d_page.evaluate("() => document.body.innerText"))
            views = view_match.group(1) if view_match else "-"
            
            data.append([f"{i+1}ìœ„", "ì¹´ì¹´ì˜¤í˜ì´ì§€", title, author, genre, views, thumbnail, "2026-02-25"])
            d_page.close()
        except: continue
    page.close()
    return data

def get_naver_data(context):
    print("      ë„¤ì´ë²„ ì‹œë¦¬ì¦ˆ ìˆ˜ì§‘ ì¤‘...")
    data = []
    page = context.new_page()
    
    # ë„¤ì´ë²„ ì‹¤ì‹œê°„ TOP 100 (ì „ì²´)
    url = "https://series.naver.com/novel/top100List.series?rankingTypeCode=REALTIME&categoryCode=ALL"
    
    try:
        # 1. í˜ì´ì§€ ì ‘ì† ë° ë¡œë”© ëŒ€ê¸°
        page.goto(url, wait_until="networkidle")
        page.wait_for_timeout(3000) # ë¦¬ìŠ¤íŠ¸ê°€ ì™„ì „íˆ ë¿Œë ¤ì§ˆ ë•Œê¹Œì§€ ëŒ€ê¸°
        
        # 2. ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ì„ íƒ (li íƒœê·¸)
        items = page.locator('div.lst_list_wrap > ul > li').all()
        print(f"      ë„¤ì´ë²„ ì•„ì´í…œ ë°œê²¬: {len(items)}ê°œ")

        for i, item in enumerate(items[:20]):
            try:
                # 3. ìƒì„¸ ì •ë³´ ì¶”ì¶œ
                # ì œëª©
                title_el = item.locator('h3 > a')
                title = title_el.inner_text().strip()
                
                # ì‘ê°€ì™€ ì¥ë¥´ (ë³´í†µ "ì‘ê°€ëª… | ì¥ë¥´" í˜¹ì€ ë³„ë„ span)
                author = item.locator('span.author').inner_text().strip().replace("ì €", "").strip()
                genre = item.locator('span.genre').inner_text().strip()
                
                # ë³„ì  (ì¡°íšŒìˆ˜ ëŒ€ìš©)
                score = item.locator('em.score_num').inner_text().strip()
                views = f"ë³„ì  {score}"
                
                # ì¸ë„¤ì¼ (ë„¤ì´ë²„ëŠ” lazy loadingì´ ìˆì–´ data-srcë‚˜ src í™•ì¸)
                img_el = item.locator('img')
                thumbnail = img_el.get_attribute("src")
                if "blank.gif" in thumbnail: # ì‹¤ì œ ì´ë¯¸ì§€ê°€ ë¡œë”© ì „ì´ë¼ë©´
                    thumbnail = img_el.get_attribute("data-src")

                data.append([
                    f"{i+1}ìœ„", 
                    "ë„¤ì´ë²„ ì‹œë¦¬ì¦ˆ", 
                    title, 
                    author, 
                    genre, 
                    views, 
                    thumbnail, 
                    "2026-02-25"
                ])
                print(f"      âœ… ë„¤ì´ë²„ {i+1}ìœ„ ì™„ë£Œ: {title}")
            except Exception as e:
                print(f"      âš ï¸ ë„¤ì´ë²„ {i+1}ìœ„ ìˆ˜ì§‘ ì¤‘ ê°œë³„ ì˜¤ë¥˜: {e}")
                continue
    except Exception as e:
        print(f"      âŒ ë„¤ì´ë²„ í˜ì´ì§€ ì ‘ì† ì˜¤ë¥˜: {e}")
    
    page.close()
    return data
def run_total_ranking():
    print("ğŸš€ [ì¹´ì¹´ì˜¤ x ë„¤ì´ë²„] í†µí•© ë­í‚¹ ìˆ˜ì§‘ ì‹œì‘...")
    
    try:
        creds = json.loads(os.environ['GOOGLE_CREDENTIALS'])
        gc = gspread.service_account_from_dict(creds)
        sh = gc.open_by_key("1c2ax0-3t70NxvxL-cXeOCz9NYnSC9OhrzC0IOWSe5Lc").sheet1
    except Exception as e:
        print(f"âŒ ì—°ê²° ì‹¤íŒ¨: {e}"); return

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
        
        # ë°ì´í„° ìˆ˜ì§‘
        kakao_list = get_kakao_data(context)
        naver_list = get_naver_data(context)
        
        # í•©ì¹˜ê¸°
        header = [["ìˆœìœ„", "í”Œë«í¼", "íƒ€ì´í‹€", "ì‘ê°€", "ì¥ë¥´", "ì¡°íšŒìˆ˜", "ì¸ë„¤ì¼", "ìˆ˜ì§‘ì¼"]]
        final_data = header + kakao_list + naver_list
        
        # ì‹œíŠ¸ ì—…ë°ì´íŠ¸
        sh.clear()
        sh.update('A1', final_data)
        print(f"ğŸŠ ì´ {len(final_data)-1}ê°œì˜ ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
        browser.close()

if __name__ == "__main__":
    run_total_ranking()
